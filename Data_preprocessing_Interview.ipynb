{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF6aBkzqXj5Z"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTKnASii3HHg"
      },
      "outputs": [],
      "source": [
        "!pip install gensim==3.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbCLSxrnu56Z"
      },
      "outputs": [],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1AdqSXCyV1j"
      },
      "outputs": [],
      "source": [
        "!pip install natasha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dPtIDDMXIbO"
      },
      "outputs": [],
      "source": [
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1HeSr0WXIbO"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zslny3JBXIbO"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download ru_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gth7D9lcyTlH"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3hY-n8CWwEE"
      },
      "outputs": [],
      "source": [
        "import nltk.data \n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOggfkYZvByy",
        "outputId": "508b8ee3-1a8b-4615-a4cd-3d28b9e98713"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "from scipy.special import logsumexp\n",
        "import gensim.models.phrases\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from docx import Document\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from spacy.lang.ru.stop_words import STOP_WORDS\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from wordcloud import (WordCloud, get_single_color_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc0L9QOpz__v"
      },
      "outputs": [],
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    \n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "    \n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "\n",
        "    Doc\n",
        ")\n",
        "\n",
        "\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "names_extractor = NamesExtractor(morph_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9xjkZITXIbQ"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4RE7ipEXIbQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sismetanin/rubert-ru-sentiment-rusentiment\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"sismetanin/rubert-ru-sentiment-rusentiment\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    predicted = torch.argmax(predicted, dim=1).numpy()\n",
        "    return predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfmZ9QtFhRup"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS4HjgU9u-6v"
      },
      "outputs": [],
      "source": [
        "def getText(filename): \n",
        "    doc = Document(filename)\n",
        "    fullText = []\n",
        "    for para in doc.paragraphs:\n",
        "        fullText.append(para.text)\n",
        "    return '\\n '.join(fullText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxBHphqMXIbR"
      },
      "outputs": [],
      "source": [
        "def normalize_tokens(df): #normalization of tokens in the dataset\n",
        "    normal = []\n",
        "    for i in tqdm(df['Text']):\n",
        "        t = []\n",
        "        counter = 0\n",
        "        i = Doc(i)\n",
        "        i.segment(segmenter)\n",
        "        i.tag_morph(morph_tagger)\n",
        "        for token in i.tokens:\n",
        "            token.lemmatize(morph_vocab)\n",
        "            t.append(list(token)[-1])\n",
        "        normal.append(t)\n",
        "    return normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1mq6ZRWXIbS"
      },
      "outputs": [],
      "source": [
        "def get_tags(df): #retrieving named entities and the number of times they occur in a given array of interviews\n",
        "    tags = []\n",
        "    for i in tqdm(df['Text']):\n",
        "        i = Doc(i)\n",
        "        i.segment(segmenter)\n",
        "        i.tag_morph(morph_tagger)\n",
        "        for token in i.tokens:\n",
        "            token.lemmatize(morph_vocab)\n",
        "        i.tag_ner(ner_tagger)\n",
        "        for span in i.spans:\n",
        "            span.normalize(morph_vocab)\n",
        "        if i.ner.spans != []:\n",
        "            tags.append(i.spans[0])\n",
        "    \n",
        "    tags_count = {}\n",
        "    for tag in range(len(tags)): \n",
        "        tag = list(tags[tag])[-2]\n",
        "        if tag not in tags_count.keys():\n",
        "            tags_count[tag]=1\n",
        "        else:\n",
        "            tags_count[tag]=tags_count[tag]+1\n",
        "    \n",
        "    tcs = sorted(tags_count.items(), key=lambda x: x[1], reverse = True)\n",
        "    \n",
        "    return tcs,tags_count \n",
        "#tcs - list of arrays of type (named entity, frequency of occurrency) sorted by frequency\n",
        "#tags_count - dictionary, where keys are named entities and values are their frequencies of occurrence in the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4Re_w_rXIbS"
      },
      "outputs": [],
      "source": [
        "def filterSent(normalized_tokens): #removing stop-words\n",
        "    filtered_sentences =[]\n",
        "    stroka = ''\n",
        "    for doc in tqdm(normalized_tokens):\n",
        "        a = []\n",
        "        for i in doc:\n",
        "            if i not in STOP_WORDS:\n",
        "                for n in i:\n",
        "                    if n.isdigit()==False:\n",
        "                        continue\n",
        "                    else:\n",
        "                        i = i.replace(n, '')\n",
        "                a.append(i)\n",
        "                stroka = stroka + i + ' '\n",
        "        filtered_sentences.append(a)\n",
        "    return filtered_sentences, stroka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzljvMKqXIbS"
      },
      "outputs": [],
      "source": [
        "def bigram(filtered_sentences): #creating 2-word collocations if words occur together in the same order more than 4 times\n",
        "    phrases = Phrases(filtered_sentences, min_count=4, progress_per=10000)\n",
        "    phraser = Phraser(phrases)\n",
        "    bigram_sentences = phraser[filtered_sentences]\n",
        "    \n",
        "    stroka = ''\n",
        "    word_freq = defaultdict(int)\n",
        "    for sent in bigram_sentences:\n",
        "        for i in sent:\n",
        "            word_freq[i] += 1\n",
        "            stroka = stroka + i + ' '\n",
        "\n",
        "    freq_sorted = sorted(word_freq.items(), key=lambda x: x[1], reverse = True)\n",
        "    \n",
        "    return freq_sorted, phraser[filtered_sentences], stroka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAeNd32zXIbS"
      },
      "outputs": [],
      "source": [
        "def top_sent_word(df_pred):\n",
        "    df_sent = pd.DataFrame(columns = {'Word',\n",
        "                             'Sentiment',\n",
        "                             'Sentence',\n",
        "                                  'Filtered'})\n",
        "    count = {}\n",
        "    for name in names:\n",
        "        pos = 0\n",
        "        neg = 0\n",
        "        for pred in range(len(df_pred['Text'])):\n",
        "                if name in df_pred['Text'][pred]:\n",
        "                    if df_pred['Sentiment label'][pred] == 2:\n",
        "                        pos+=1\n",
        "                        df_sent = df_sent.append({'Word': name,\n",
        "                                        'Sentiment': 2, \n",
        "                                        'Sentence': df_pred['Sentence'][pred],\n",
        "                                                 'Filtered':df_pred['Text'][pred]}, \n",
        "                                       ignore_index = True)\n",
        "                    elif df_pred['Sentiment label'][pred] == 0:\n",
        "                        neg+=1\n",
        "                        df_sent = df_sent.append({'Word':name, \n",
        "                                        'Sentiment': 0, \n",
        "                                        'Sentence': df_pred['Sentence'][pred],\n",
        "                                                 'Filtered':df_pred['Text'][pred]}, \n",
        "                                       ignore_index = True)\n",
        "        if pos !=0 or neg !=0:\n",
        "            count[name] = [pos,neg]\n",
        "        \n",
        "    neg_word = {}\n",
        "    pos_word = {}\n",
        "    avg = {}\n",
        "    for i in range(len(df_sent['Filtered'])):\n",
        "        for m in df_sent['Filtered'][i]:\n",
        "            if m not in names:\n",
        "                if df_sent[\"Sentiment\"][i] == 0:\n",
        "                    if m not in neg_word.keys():\n",
        "                        neg_word[m] = 1\n",
        "                    else:\n",
        "                        neg_word[m] = neg_word[m]+1\n",
        "\n",
        "                if df_sent[\"Sentiment\"][i] == 2:\n",
        "                    if m not in pos_word.keys():\n",
        "                        pos_word[m] = 1\n",
        "                    else:\n",
        "                        pos_word[m] = pos_word[m]+1\n",
        "                        \n",
        "    pos = sorted(pos_word.items(), key=lambda x: x[1], reverse = True)\n",
        "    neg = sorted(neg_word.items(), key=lambda x: x[1], reverse = True)\n",
        "    \n",
        "        \n",
        "    return pos, neg, count, neg_word, pos_word\n",
        "#pos - sorted ranking of words with most positive sentiments\n",
        "#neg - sorted ranking of words with most negative sentiments\n",
        "#count - dictionary, where keys are words and values are lists of type [freq. of positive sentiment, freq. of negative sentiment]\n",
        "#neg_word - unsorted dictionary, where keys are words and values are frequencies of negative sentiment\n",
        "#pos_word - unsorted dictionary, where keys are words and values are frequencies of positive sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFAQjrOUXIbT"
      },
      "outputs": [],
      "source": [
        "def bar_h(bigram_sentences):\n",
        "    names = []\n",
        "    values = []\n",
        "    f = bigram_sentences[0][:30]\n",
        "    for i in f:\n",
        "        names.append(i[0])\n",
        "        values.append(i[1])\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.barh(range(len(values)), values, tick_label=names)\n",
        "    plt.yticks(fontsize = 16)\n",
        "    plt.xticks(fontsize = 14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "#simple histogram illustrating the most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbhH-59FXIbU"
      },
      "outputs": [],
      "source": [
        "def colors(pos,neg): #color function for wordclouds\n",
        "    color_to_words = {\n",
        "        # words below will be colored with a green single color function\n",
        "        '#90EE90': pos,\n",
        "        # will be colored with a red single color function\n",
        "        '#ff5349': neg}\n",
        "    default_color = 'grey'\n",
        "    return color_to_words, default_color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2indBYvvxL5O"
      },
      "outputs": [],
      "source": [
        "def get_df(number, folder):\n",
        "  folder_url = folder\n",
        "  file_url = 'Инт {}.docx'.format(number)\n",
        "  url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download' + '?public_key=' + urllib.parse.quote(folder_url) + '&path=/' + urllib.parse.quote(file_url)\n",
        "\n",
        "  r = requests.get(url)\n",
        "  download_url = r.json()['href']\n",
        "\n",
        "  download_response = requests.get(download_url)\n",
        "  with open('file_{}.txt'.format(number), 'wb') as f: \n",
        "    f.write(download_response.content)\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "  text = getText('/content/file_{}.txt'.format(number))\n",
        "  doc = Doc(text)\n",
        "  doc.segment(segmenter)\n",
        "  counter = 0\n",
        "  step_e = 0\n",
        "  for m in range(len(doc.sents)):\n",
        "        if 'Эксперт' in doc.sents[m].text:\n",
        "            counter +=1\n",
        "            step_e += 1\n",
        "        if step_e !=0:\n",
        "            if not 'Интервьюер' in doc.sents[m].text:\n",
        "                fint = doc.sents[m].text.lstrip('Эксперт')\n",
        "                step_e+=1\n",
        "            else:\n",
        "                step_e = 0\n",
        "                continue\n",
        "            \n",
        "            df = df.append({\n",
        "              'Номер интервью': number,\n",
        "              \"Номер вопроса\": counter, \n",
        "              \"Предложение\": fint}, ignore_index=True)\n",
        "        \n",
        "        if step_e == 0:\n",
        "            continue\n",
        "\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv0qNqhiXIbo"
      },
      "source": [
        "# Analysis example for one country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7-YzM_KXIbo",
        "outputId": "cc2ad71f-f53d-4205-ae0f-9da6476bc807"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:03<00:00,  3.17s/it]\n"
          ]
        }
      ],
      "source": [
        "#retrieving data from raw docx files\n",
        "df_1 = pd.DataFrame()\n",
        "for i in tqdm(range(1,21)):\n",
        "  df = get_df(str(i), ###link to folder)\n",
        "  df_1 = df_1.append(df, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR5aClTpXIbp"
      },
      "outputs": [],
      "source": [
        "t_1 = get_tags(df_1)\n",
        "n_1 = normalize_tokens(df_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgzJ0zeLXIbp"
      },
      "outputs": [],
      "source": [
        "filt_1 = filterSent(n_1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx-kevANXIbp"
      },
      "outputs": [],
      "source": [
        "b_1 = bigram(filt_1)\n",
        "text_1 = filterSent(n_1)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwScyxuAXIbp"
      },
      "outputs": [],
      "source": [
        "predicted = []\n",
        "for i in tqdm(df_1['Text']):\n",
        "    pred = predict(i)\n",
        "    predicted.append(*pred)\n",
        "df_pred = pd.DataFrame()\n",
        "df_pred['Text'] = filt_1\n",
        "df_pred['Sentiment label'] = predicted\n",
        "df_pred['Текст'] = df_1['Text']\n",
        "print(df_pred['Sentiment label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFMCTPHLXIbp"
      },
      "outputs": [],
      "source": [
        "pos_1_text = ''\n",
        "for i in df_pred[df_pred['Sentiment label']==2]['Text']:\n",
        "    for m in i:\n",
        "        pos_1_text = pos_1_text + m + ' '\n",
        "\n",
        "wordcloud_pos_1 = WordCloud(stopwords=STOP_WORDS,\n",
        "                      font_path = '/content/Lato-Black.ttf',\n",
        "                      background_color=\"white\", \n",
        "                      max_words=30, \n",
        "                      max_font_size=30,\n",
        "                      width = 400,\n",
        "                      height = 200,\n",
        "                      colormap = 'Greens'\n",
        "                     ).generate(pos_1_text)\n",
        "plt.imshow(wordcloud_pos_1, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "with open(\"1_pos.svg\", \"w\") as fp:\n",
        "    fp.write(wordcloud_pos_1.to_svg())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlfRS65DXIbp"
      },
      "outputs": [],
      "source": [
        "neg_1_text = ''\n",
        "for i in df_pred[df_pred['Sentiment label']==0]['Text']:\n",
        "    for m in i:\n",
        "        neg_1_text = neg_1_text + m + ' '\n",
        "STOP_WORDS.update(['т'])\n",
        "wordcloud_neg_1 = WordCloud(stopwords=STOP_WORDS,\n",
        "                      font_path = '/content/Lato-Black.ttf',\n",
        "                      background_color=\"white\", \n",
        "                      max_words=30, \n",
        "                      max_font_size=30,\n",
        "                      width = 400,\n",
        "                      height = 200,\n",
        "                      colormap = 'Reds'\n",
        "                     ).generate(neg_1_text)\n",
        "plt.imshow(wordcloud_neg_1, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "with open(\"1_neg.svg\", \"w\") as fp:\n",
        "    fp.write(wordcloud_neg_1.to_svg())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD2nrtD7GuOJ"
      },
      "outputs": [],
      "source": [
        "bar_h(b_kz)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nF6aBkzqXj5Z"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}