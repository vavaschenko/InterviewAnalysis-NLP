{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and functions"
      ],
      "metadata": {
        "id": "Mn34Us9AOWSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha"
      ],
      "metadata": {
        "id": "2D_AuE60LZ9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preSYKI-oqnK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import re\n",
        "import requests\n",
        "import logging\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "import nltk.data \n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from spacy.lang.ru.stop_words import STOP_WORDS\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from networkx.algorithms.community import greedy_modularity_communities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DcM71XWoqnV"
      },
      "outputs": [],
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    \n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "    \n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "\n",
        "    Doc\n",
        ")\n",
        "\n",
        "\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "names_extractor = NamesExtractor(morph_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBL4lt3eoqnd"
      },
      "outputs": [],
      "source": [
        "def to_wordlist(sentences, tokenizer, remove_stopwords=False):\n",
        "    wordlist = []\n",
        "    for sent in tqdm(sentences):\n",
        "        tokens = []\n",
        "        sent = Doc(sent)\n",
        "        sent.segment(segmenter)\n",
        "        sent.tag_morph(morph_tagger)\n",
        "        for token in sent.tokens:\n",
        "            token.lemmatize(morph_vocab)\n",
        "            token = list(token)[-1]\n",
        "            tokens.append(token)\n",
        "        \n",
        "        if remove_stopwords: # убираем стоп-слова\n",
        "            stop = STOP_WORDS\n",
        "            words = [w for w in tokens if not w in stop]\n",
        "            wordlist.append(words)\n",
        "        else: \n",
        "            wordlist.append(tokens)\n",
        "    return(wordlist)\n",
        "\n",
        "def to_sentences(df, tokenizer):\n",
        "    sents = []\n",
        "    for article in tqdm(df['Text']):\n",
        "        sentences = tokenizer.tokenize(article.strip())\n",
        "        for sent in sentences:\n",
        "            sents.append(sent)\n",
        "    return(sents)\n",
        "\n",
        "def preprocess(sentences):\n",
        "  sents = []\n",
        "  for i in sentences:\n",
        "      n = i.strip('[').rstrip(']').split(',')\n",
        "      s = []\n",
        "      for m in n:\n",
        "          m = m.lstrip('\"').lstrip(\" '\").rstrip(\"'\")\n",
        "          s.append(m)\n",
        "      sents.append(s)\n",
        "  return(sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxHj2w1moqnR"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.MinMaxScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea-FNBufoqng"
      },
      "outputs": [],
      "source": [
        "tokenizer = nltk.data.load('tokenizers/punkt/russian.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RBC & Lenta.ru: data"
      ],
      "metadata": {
        "id": "vdKI0CZqOdp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this file, already collected and preprocessed data is used\n",
        "for further information on the process of data collection please look into the 'data_collection.ipynb' file in this repository"
      ],
      "metadata": {
        "id": "kEYBuYxVA_uR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T5RXb-joqnj"
      },
      "outputs": [],
      "source": [
        "#retreiving dataset of articles from RBC\n",
        "folder_url = #######\n",
        "file_url = 'df_rbc_filtered.csv'\n",
        "url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download' + '?public_key=' + urllib.parse.quote(folder_url) + '&path=/' + urllib.parse.quote(file_url)\n",
        "\n",
        "r = requests.get(url) \n",
        "h = json.loads(r.text)['href'] \n",
        "\n",
        "rbc = pd.read_csv(h).reset_index(drop=True)\n",
        "\n",
        "#retreiving dataset of articles from Lenta.ru\n",
        "folder_url_2 = #######\n",
        "file_url_2 = 'df_lenta_filtered.csv'\n",
        "url_2 = 'https://cloud-api.yandex.net/v1/disk/public/resources/download' + '?public_key=' + urllib.parse.quote(folder_url_2) + '&path=/' + urllib.parse.quote(file_url_2)\n",
        "\n",
        "r_2 = requests.get(url_2) \n",
        "h_2 = json.loads(r_2.text)['href'] \n",
        "\n",
        "lenta = pd.read_csv(h_2).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCk0bT-soqnp"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame()\n",
        "df['Text'] = pd.concat([rbc['Text'], lenta['Text']], ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCZQyylSoqnq"
      },
      "outputs": [],
      "source": [
        "sentences = to_sentences(df, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = to_wordlist(sentences, tokenizer, remove_stopwords = True)"
      ],
      "metadata": {
        "id": "tsBSTC0xLzVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "6OJdEx0nLEbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = pd.read_excel('/content/words_model 2.csv')[0] #reading the data\n",
        "sents = preprocess(sentences) #applying the previously defined function for preprocessing"
      ],
      "metadata": {
        "id": "INisF9RxpClh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = Phrases(sents, min_count=4) #joining pairs of words that co-occur together in the same order more than 4 times into bigrams added to our vocabulary\n",
        "trigram = Phrases(bigram[sents], min_count=4) #performing the same operation but for trigrams (3-word collocations)\n",
        "\n",
        "model_bi = word2vec.Word2Vec(trigram[sents], workers=4, size=300, min_count=10, window=10, sample=1e-3) #training the model on the renewed dataset"
      ],
      "metadata": {
        "id": "qd-uzpWmsItF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the data the model was trained in was international relations and politics, we can manually check whether the similarities and dissimilarities between vectors in the model's vocabulary accord with common sense"
      ],
      "metadata": {
        "id": "yHQPUtREHtQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_bi.wv.most_similar(positive=[\"самопровозглашенный\"], negative=[\"россия\"], topn=1))\n",
        "print(model_bi.wv.most_similar(\"россия\", topn=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNVCl1NHxHRW",
        "outputId": "45f5a415-e5be-4c25-8443-a3a08ac7a790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('республика_днр', 0.6437524557113647)]\n",
            "[('рф', 0.4528544545173645), ('москва', 0.42486968636512756), ('российский', 0.36448073387145996)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_bi.wv.save('vectors.kv')\n",
        "reloaded_word_vectors = KeyedVectors.load('vectors.kv')"
      ],
      "metadata": {
        "id": "QGJTv9TeEndA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"news.model\"\n",
        "\n",
        "print(\"Saving model...\")\n",
        "model_bi.save(model_path)"
      ],
      "metadata": {
        "id": "coGerMuVv_hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning and visualizations"
      ],
      "metadata": {
        "id": "WHEyIJDXKzlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "ZahxxnQlvT7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'news.model'\n",
        "!wget ### #retrieving the model from a remote repository"
      ],
      "metadata": {
        "id": "xaic7P7lQ-UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis example for one country"
      ],
      "metadata": {
        "id": "oPJQ7V7QXfmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = word2vec.Word2Vec.load(model_path) #the baseline model is loaded\n",
        "\n",
        "sentences_1 = to_sentences(df_1, tokenizer) #interview texts for the country are segmented into sentences\n",
        "words_1 = to_wordlist(sentences_1, tokenizer, remove_stopwords = True) #sentences are segmented into words\n",
        "\n",
        "bigram = Phrases(words_1, min_count=3) #bigrams are formed (due to lower amount of texts, 3 cases of cooccurrence are taken to mean that the pair is a collocation)\n",
        "trigram = Phrases(bigram[words_1], min_count=3) #trigrams are formed\n",
        "\n",
        "model_1.build_vocab(trigram[words_1], update=True) #vocabulary is updated\n",
        "model_1.train(trigram[words_1], total_examples=model_1.corpus_count, epochs=50) #the model is trained to fit interview data"
      ],
      "metadata": {
        "id": "j3VaZJgSxkt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_1.wv.most_similar(\"российский\", topn=10)) #sanity check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSaO0yZfzU5V",
        "outputId": "6f27b96b-917d-4cce-bdef-8a86cf28eb62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('россия', 0.8896118402481079), ('абхазия', 0.8636842370033264), ('культура', 0.8600834608078003), ('сказать', 0.8559765815734863), ('спорт', 0.8559607267379761), ('интервью', 0.842548131942749), ('местный', 0.8394073247909546), ('оказывать_поддержка', 0.8354763388633728), ('спрашивать', 0.8348627090454102), ('субъект', 0.8347859382629395)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph visualization"
      ],
      "metadata": {
        "id": "VW_12U-qkprD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dummy_preprocessor(doc):\n",
        "    return doc\n",
        "def whitelist_preprocessor(doc):\n",
        "    return [x for x in doc if x in whitelist]\n",
        "\n",
        "def get_colors(communities): \n",
        "  colors = ['b', 'g', 'r', 'c', '#c20078', 'y', 'k', \n",
        "          '#f97306'] + ['b']*(len(communities)-8)\n",
        "  return(colors)\n",
        "#this is a dummy solution: we hypothesize that, \n",
        "#given that only 100 words will be visualized, \n",
        "#it is unlikely that more than 8 communities will be present\n",
        "#there are 8 defined colors for 8 distinct communities\n",
        "#the rest of the communities, should there be any, are filled with blue by default\n",
        "\n",
        "def words_for_pics(corpus):\n",
        "  tfidf_vectorizer = TfidfVectorizer(\n",
        "        tokenizer=dummy_preprocessor,\n",
        "        preprocessor=dummy_preprocessor,\n",
        "        max_df=0.95, min_df=2,\n",
        "        max_features=100\n",
        "    )\n",
        "  corpus = corpus\n",
        "  X = tfidf_vectorizer.fit_transform(corpus)\n",
        "  importance = np.argsort(np.asarray(X.sum(axis=0)).ravel())[::-1]\n",
        "  tf_feature_names = np.array(tfidf_vectorizer.get_feature_names())\n",
        "  whitelist = tf_feature_names[importance[:100]].tolist()\n",
        "  whitelist = set(whitelist)\n",
        "  return(whitelist)\n",
        "#top-100 most important words in the corpus are selected via TF-IDF vectorization\n",
        "\n",
        "def vecs_for_pics(corpus, whitelist,max_df, min_df, tr):\n",
        "  count_vectorizer = CountVectorizer(\n",
        "        tokenizer=whitelist_preprocessor,\n",
        "        preprocessor=whitelist_preprocessor,\n",
        "        max_df=max_df, min_df=min_df)\n",
        "\n",
        "  mat = count_vectorizer.fit_transform(corpus)\n",
        "  feature_names = np.array(count_vectorizer.get_feature_names()).tolist()\n",
        "\n",
        "  mat = np.transpose(mat.toarray())\n",
        "  A = np.corrcoef(mat)\n",
        "  np.fill_diagonal(A, 0.0)\n",
        "  A[A < tr] = 0\n",
        "  return(A, feature_names)\n",
        "#here we use count vectorizer to create vectors for selected words \n",
        "#we assume that if patterns of cooccurrence are similar between two words than the words themselves are similar\n",
        "#a threshold 'tr' is passed to the function to define the value under which we consider correlations between created vectors to be insignificant and turn them to zero\n",
        "#we also want the words we are going to visualize to not be too common or too rare in the corpus, so we pass limits on min and max occurrences in the corpus\n",
        "\n",
        "def net_graph(A, feature_names, short_country_name, h):\n",
        "  G = nx.Graph(A)\n",
        "  communities = greedy_modularity_communities(G)\n",
        "  coord = nx.spring_layout(G, k = 0.15)\n",
        "\n",
        "  plt.figure(figsize=(h,h))\n",
        "\n",
        "  labels = dict(enumerate(feature_names))\n",
        "  colors = get_colors(communities)\n",
        "  aux = 0\n",
        "  for community in communities:\n",
        "    nx.draw_networkx_nodes(G, coord, community, node_size = 10, node_color = colors[aux])\n",
        "  \n",
        "    label_dict = {k: v for (k,v) in labels.items() if k in set(community)}\n",
        "    nx.draw_networkx_labels(G, coord, labels=label_dict, font_color=colors[aux], \n",
        "                            font_size = 13, \n",
        "                            font_family = 'Liberation Sans')\n",
        "    aux = aux + 1\n",
        "  \n",
        "  nx.draw_networkx_edges(G, pos=coord, alpha=0.5, edge_color = 'grey')\n",
        "  plt.title(\"{} discource graph\".format(short_country_name.capitalize()))\n",
        "  plt.savefig(\"{}.svg\".format(short_country_name), format=\"svg\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "kuRs9sbsLDQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whitelist_1 = words_for_pics(trigram[words_1])"
      ],
      "metadata": {
        "id": "akTNNiRjnZbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_1, fn_1 = vecs_for_pics(trigram[words_1], whitelist_1,0.95,3,0.5)\n",
        "#data matrix and feature names for the example country are defined\n",
        "#words in the final visualization will be occurring in less than 95% of the sentences in our corpus but no less than 3 times\n",
        "#to be present on the graph as a link between two words, their vectors have to be strongly correlated (corrcoef > 0.5)"
      ],
      "metadata": {
        "id": "Ir-zkBPFoL5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_graph(A_1, #matrix\n",
        "          fn_1, #feature names\n",
        "          '1', #name of the group\n",
        "          25 #height of the image\n",
        "          )"
      ],
      "metadata": {
        "id": "l2bSE3MYq294"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vdKI0CZqOdp3",
        "6OJdEx0nLEbE",
        "eqpI-HCeKxvW",
        "JfqjR-o88NY8",
        "Wl_yeAvY-Ex5",
        "ZnkzM3OFAYoz",
        "V95A4wxA-XTE",
        "kAQouSDQCVIZ",
        "vH47f02KVDfw"
      ],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}